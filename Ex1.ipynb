{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "# evaluation\n",
    "from sklearn.model_selection import KFold\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv-file into dataframe df\n",
    "csv_data_file = 'Exercise 1 - Transaction Classification - Data Set.csv'\n",
    "df = pd.read_csv(csv_data_file, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Auftragskonto</th>\n",
       "      <th>Buchungstag</th>\n",
       "      <th>Valutadatum</th>\n",
       "      <th>Buchungstext</th>\n",
       "      <th>Verwendungszweck</th>\n",
       "      <th>Beguenstigter/Zahlungspflichtiger</th>\n",
       "      <th>Kontonummer</th>\n",
       "      <th>BLZ</th>\n",
       "      <th>Betrag</th>\n",
       "      <th>Waehrung</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>89990201.0</td>\n",
       "      <td>28.07.2016</td>\n",
       "      <td>28.07.2016</td>\n",
       "      <td>Lohn / Gehalt</td>\n",
       "      <td>Gehalt Adorsys GmbH &amp; Co. KG End-To-End-Ref.: ...</td>\n",
       "      <td>Adorsys GmbH &amp; Co. KG</td>\n",
       "      <td>7807800780</td>\n",
       "      <td>25190001</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>EUR</td>\n",
       "      <td>income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>89990201.0</td>\n",
       "      <td>27.07.2016</td>\n",
       "      <td>27.07.2016</td>\n",
       "      <td>Miete</td>\n",
       "      <td>Byladem1Sbt De12773501123456789889 Miete Beuth...</td>\n",
       "      <td>Georg Tasche</td>\n",
       "      <td>DE31251900019123456780</td>\n",
       "      <td>VOHADE2HXXX</td>\n",
       "      <td>-670.00</td>\n",
       "      <td>EUR</td>\n",
       "      <td>living</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>89990201.0</td>\n",
       "      <td>21.07.2016</td>\n",
       "      <td>21.07.2016</td>\n",
       "      <td>Bargeld</td>\n",
       "      <td>21.07/16.34Uhr Nuernberg All Eur 70,00 Geb.Eur...</td>\n",
       "      <td>Bargeld</td>\n",
       "      <td>9999900780</td>\n",
       "      <td>25190001</td>\n",
       "      <td>-70.00</td>\n",
       "      <td>EUR</td>\n",
       "      <td>private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>89990201.0</td>\n",
       "      <td>20.07.2016</td>\n",
       "      <td>20.07.2016</td>\n",
       "      <td>Lebensmittel / Getraenke</td>\n",
       "      <td>2831 Edeka Neubauer Nuernb.//Nuernb 2016-07-20...</td>\n",
       "      <td>Kartenzahlung</td>\n",
       "      <td>9736000780</td>\n",
       "      <td>25190001</td>\n",
       "      <td>-73.21</td>\n",
       "      <td>EUR</td>\n",
       "      <td>standardOfLiving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>89990201.0</td>\n",
       "      <td>18.07.2016</td>\n",
       "      <td>18.07.2016</td>\n",
       "      <td>Spontanausgabe</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>neue Playstation</td>\n",
       "      <td>9988776655</td>\n",
       "      <td>25125100</td>\n",
       "      <td>-363</td>\n",
       "      <td>EUR</td>\n",
       "      <td>leisure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Auftragskonto Buchungstag Valutadatum  \\\n",
       "0           0     89990201.0  28.07.2016  28.07.2016   \n",
       "1           1     89990201.0  27.07.2016  27.07.2016   \n",
       "2           2     89990201.0  21.07.2016  21.07.2016   \n",
       "3           3     89990201.0  20.07.2016  20.07.2016   \n",
       "4           4     89990201.0  18.07.2016  18.07.2016   \n",
       "\n",
       "               Buchungstext  \\\n",
       "0             Lohn / Gehalt   \n",
       "1                     Miete   \n",
       "2                   Bargeld   \n",
       "3  Lebensmittel / Getraenke   \n",
       "4            Spontanausgabe   \n",
       "\n",
       "                                    Verwendungszweck  \\\n",
       "0  Gehalt Adorsys GmbH & Co. KG End-To-End-Ref.: ...   \n",
       "1  Byladem1Sbt De12773501123456789889 Miete Beuth...   \n",
       "2  21.07/16.34Uhr Nuernberg All Eur 70,00 Geb.Eur...   \n",
       "3  2831 Edeka Neubauer Nuernb.//Nuernb 2016-07-20...   \n",
       "4                                             Amazon   \n",
       "\n",
       "  Beguenstigter/Zahlungspflichtiger             Kontonummer          BLZ  \\\n",
       "0             Adorsys GmbH & Co. KG              7807800780     25190001   \n",
       "1                      Georg Tasche  DE31251900019123456780  VOHADE2HXXX   \n",
       "2                           Bargeld              9999900780     25190001   \n",
       "3                     Kartenzahlung              9736000780     25190001   \n",
       "4                  neue Playstation              9988776655     25125100   \n",
       "\n",
       "    Betrag Waehrung             label  \n",
       "0  2000.00      EUR            income  \n",
       "1  -670.00      EUR            living  \n",
       "2   -70.00      EUR           private  \n",
       "3   -73.21      EUR  standardOfLiving  \n",
       "4     -363      EUR           leisure  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label               count\n",
      "-------------------------\n",
      "leisure             65\n",
      "standardOfLiving    47\n",
      "finance             33\n",
      "living              26\n",
      "private             21\n",
      "income              17\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# distribution of categories/labels\n",
    "print(f\"label{' ' * 15}count\")\n",
    "print(f\"{'-' * 25}\")\n",
    "print(f\"{df.label.value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['accumulated_data'] => accumalated columns 'Buchungstext', 'Verwendungszweck'and 'Beguenstigter/Zahlungspflichtiger'\n",
    "# df['preprocessed_data'] => df['accumulated_data']; no numbers / no digit-characters\n",
    "df['accumulated_data'] = df[['Buchungstext', 'Verwendungszweck', 'Beguenstigter/Zahlungspflichtiger']]. \\\n",
    "    apply(lambda x: ' '.join(x), axis=1)\n",
    "pattern = re.compile('\\d')\n",
    "df['preprocessed_data'] = df['accumulated_data'].apply(lambda x: pattern.sub('', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423\n",
      "['00', '000', '0000000000', '001', '0084632', '00eur0', '01', '016', '0164378887929309', '01e', '01t06', '01t11', '01t17', '02', '02t13', '02t14', '02t17', '02t22', '03', '03t12', '03t16', '04', '04032016', '04t17', '05', '0580631', '05t09', '05t23', '06', '07', '07folgenr', '08', '09', '09t07', '09t17', '10', '100', '10103084736793zalando', '10316011', '10355311', '10355499', '10355735', '10355942', '10355969', '10355999', '10t17', '11', '1137', '116', '12', '12272140', '122721405', '13', '13t', '14', '140', '14t12', '15', '150244534', '150251073', '150257651', '15t17', '16', '161080109883418', '1612', '162146', '1637782719640441', '16t17', '17', '1704', '18', '18t21', '19', '19t15', '19t17', '1u1', '20', '2001136', '201', '2016', '2020', '2099', '20t15', '20t17', '21', '2103', '22', '2202002154', '220200215414', '23', '230320211004393201252017040elv6520', '23t11', '23t13', '24', '2410573888643087', '2411354884929964', '24t12', '25', '26', '26folgenr', '26t15', '27', '279', '27folgenr', '27t10', '27t11', '28', '2831', '29', '30', '303', '30eur0', '30t14', '31', '317', '32', '320000042615', '33', '330000099057', '34', '34439927', '34uhr', '35', '350193', '36', '37', '3814', '39', '40', '42', '42eur0', '43', '43folgenr', '4488385', '45', '46400517', '48', '50', '51', '5104489', '52', '5339566', '535', '54', '54eur0', '54folgenr', '55', '551', '57', '574704817', '58folgenr', '59', '6169917', '65128', '65214', '65354', '6620', '69', '70', '73eur0', '7483141', '7540', '798', '80eur0', '8324', '84eur0', '8794708', '91', '95eur0', '99eur0', 'abonnement', 'abschlag', 'abschluss', 'adorsys', 'aenderungen', 'aeu', 'ag', 'all', 'alld', 'amazon', 'ankenvers', 'anna', 'apotheke', 'aramark', 'ard', 'atm', 'august', 'aus', 'ausland', 'auslandsrei', 'auszahlung', 'bank', 'bankhaus', 'bar', 'bargeld', 'bears', 'beitrags', 'beitragsservice', 'belastung', 'bequ', 'beuthener', 'bil', 'billpay', 'bistro', 'blz76010085', 'bp1821187056', 'breuninger', 'byladem1sbt', 'cafe', 'california', 'ccb', 'ccbade31xxx', 'celona', 'christina', 'city', 'classic', 'co', 'comix', 'company', 'dank', 'danke', 'dankt', 'dauerauftrag', 'de', 'de12773501123456789889', 'de41760501010012345675', 'de71310108339900123456', 'deco', 'deutsche', 'deutschland', 'dietrich', 'digital', 'dm', 'doris', 'dradio', 'drogerie', 'drogeriemarkt', 'e1', 'edeka', 'eg', 'einfach', 'einmalsparen', 'einzahlung', 'einzugsermächtigung', 'elv56006914', 'elv56006915', 'elv56006916', 'elv56006917', 'elv66021447', 'em', 'end', 'erg', 'esso', 'eu', 'eur', 'euro', 'europe', 'euroscheck', 'fein', 'fil', 'folgenr', 'foodora', 'friends', 'fuerth', 'fuerthermare', 'ga', 'gaa', 'ganz', 'gas', 'gastro', 'gastronomie', 'gastst', 'gaststaette', 'geb', 'gehalt', 'geldanlage', 'georg', 'geschenk', 'getraenke', 'glas', 'gmb', 'gmbh', 'gries', 'gruenstrom', 'gt', 'gutschrift', 'haushalt', 'hobex', 'hoefe', 'horst', 'hs', 'humanic', 'icherung', 'im', 'interne', 'intersport', 'isabel', 'issuer', 'jung', 'k358853507', 'karolinen', 'kartenzahlung', 'kd', 'kfn', 'kg', 'koestr', 'koffer', 'kundenreferenz', 'lastschrift', 'ld', 'lebensmittel', 'lenz', 'lld', 'ller', 'lohn', 'lombagine', 'lucas', 'mango', 'marketplace', 'maxx', 'me11', 'me1rewe', 'michael', 'miete', 'mieteinnahmen', 'mietzuschuss', 'mktplce', 'moda', 'neubauer', 'neue', 'notprovided', 'nr', 'nr55508568', 'nsct1603080001510000000000000000006', 'nsct1603300013660000000000000000001', 'nuernb', 'nuernber', 'nuernberg', 'nuremberg', 'nurnberg', 'nürnberg', 'omv', 'payments', 'playstation', 'pulsnitz', 're', 'ref', 'reit', 'rent', 'rente', 'rewe', 'rfalld', 'rg', 'roethenbach', 'rse', 'rundfunk', 'rundfunkbeitrag', 'sa', 'sagt', 'sarah', 'se', 'sebalder', 'sekr', 'seon', 'skyline', 'sparen', 'sparkasse', 'spdude68xxx', 'spontanausgabe', 'ssknde77xxx', 'str', 'strom', 'stromio', 'style', 'sushi', 't19', 'tasche', 'teambank', 'telecom', 'ticket', 'tk', 'to', 'travelsecure', 'ts', 'u1', 'ue', 'ueberweisung', 'ultra', 'umbuchung', 'urlaub', 'vag', 've', 'veranst', 'verf', 'verfa', 'verfal', 'verfall', 'verfalld', 'vero', 'versicherungs', 'video', 'vielen', 'visa6218501306060876', 'visa62185013800', 'visa62185013amazon', 'visa62185013nuernberg', 'visa6218502106060876', 'visa62185021800', 'visa62185021amazon', 'visa62185021nbg', 'visa62185021rathaus', 'visa62185021schnieglin', 'visa62185021senningerberg', 'vj', 'vk', 'vr', 'wie', 'winkl', 'wuerzburger', 'www', 'zalando', 'zdf', 'überweisung']\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer with df['accumulated_data'] (no preprocessing)\n",
    "# no words to be left out\n",
    "\n",
    "count_vectorizer = CountVectorizer(lowercase=True)\n",
    "count_vectorizer.fit(df['accumulated_data'])\n",
    "# 423 features\n",
    "print(len(count_vectorizer.get_feature_names()))\n",
    "# features (the corresponding words)\n",
    "print(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221\n",
      "['abonnement', 'abschlag', 'abschluss', 'adorsys', 'aenderungen', 'ag', 'alld', 'amazon', 'ankenvers', 'anna', 'apotheke', 'aramark', 'ard', 'atm', 'august', 'aus', 'ausland', 'auslandsrei', 'auszahlung', 'bank', 'bankhaus', 'bar', 'bargeld', 'bears', 'beitrags', 'beitragsservice', 'belastung', 'bequ', 'beuthener', 'bil', 'billpay', 'bistro', 'breuninger', 'cafe', 'california', 'ccb', 'celona', 'christina', 'city', 'classic', 'co', 'comix', 'company', 'dank', 'danke', 'dankt', 'dauerauftrag', 'de', 'deco', 'deutsche', 'deutschland', 'dietrich', 'digital', 'dm', 'doris', 'dradio', 'drogerie', 'drogeriemarkt', 'edeka', 'eg', 'einfach', 'einmalsparen', 'einzahlung', 'einzugsermächtigung', 'elv', 'em', 'end', 'erg', 'esso', 'eu', 'eur', 'euro', 'europe', 'euroscheck', 'fein', 'fil', 'folgenr', 'foodora', 'friends', 'fuerth', 'fuerthermare', 'ganz', 'gas', 'gastro', 'gastronomie', 'gastst', 'gaststaette', 'gehalt', 'geldanlage', 'georg', 'geschenk', 'getraenke', 'glas', 'gmb', 'gmbh', 'gries', 'gruenstrom', 'gt', 'gutschrift', 'haushalt', 'hobex', 'hoefe', 'horst', 'hs', 'humanic', 'icherung', 'im', 'interne', 'intersport', 'isabel', 'issuer', 'jung', 'karolinen', 'kartenzahlung', 'kd', 'kfn', 'kg', 'koestr', 'koffer', 'kundenreferenz', 'lastschrift', 'ld', 'lebensmittel', 'lenz', 'lld', 'ller', 'lohn', 'lombagine', 'lucas', 'mango', 'marketplace', 'maxx', 'me', 'merewe', 'michael', 'miete', 'mieteinnahmen', 'mietzuschuss', 'mktplce', 'moda', 'neubauer', 'neue', 'notprovided', 'nr', 'nsct', 'nuernb', 'nuernber', 'nuernberg', 'nuremberg', 'nurnberg', 'nürnberg', 'omv', 'payments', 'playstation', 'pulsnitz', 're', 'ref', 'reit', 'rent', 'rente', 'rewe', 'rg', 'roethenbach', 'rse', 'rundfunk', 'rundfunkbeitrag', 'sa', 'sagt', 'sarah', 'se', 'sebalder', 'sekr', 'seon', 'skyline', 'sparen', 'sparkasse', 'spontanausgabe', 'str', 'strom', 'stromio', 'style', 'sushi', 'tasche', 'teambank', 'telecom', 'ticket', 'tk', 'travelsecure', 'ueberweisung', 'uhr', 'ultra', 'umbuchung', 'urlaub', 've', 'veranst', 'verf', 'verfa', 'verfal', 'verfall', 'verfalld', 'vero', 'versicherungs', 'video', 'vielen', 'visa', 'visaamazon', 'visanbg', 'visanuernberg', 'visarathaus', 'visaschnieglin', 'visasenningerberg', 'vj', 'vk', 'vr', 'wie', 'winkl', 'wuerzburger', 'www', 'zalando', 'zdf', 'überweisung']\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer with df['preprocessed_data'] (numbers eliminaded)\n",
    "# 16 stop words to be left out\n",
    "stop_words = ['aeu', 'all', 'bylademsbt', 'blz', 'bp', 'ccbadexxx', 'ga', 'gaa', \\\n",
    "              'geb', 'rfalld', 'spdudexxx', 'sskndexxx', 'to', 'ts', 'ue', 'vag']\n",
    "count_vectorizer = CountVectorizer(lowercase=True, stop_words=stop_words)\n",
    "count_vectorizer.fit(df['preprocessed_data'])\n",
    "# 221 features\n",
    "print(len(count_vectorizer.get_feature_names()))\n",
    "# features (the corresponding words)\n",
    "print(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform df['preprocessed_data'] to vector (necessary to train with)\n",
    "vectorized_messages = count_vectorizer.transform(df['preprocessed_data']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the Gaussian Naive Bayes Classifier with the full data set\n",
    "gnb = GaussianNB()\n",
    "gnb = gnb.fit(vectorized_messages, df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# basic test with full set as training set and full set as test set\n",
    "y_test = gnb.predict(count_vectorizer.transform(df['preprocessed_data']).toarray())\n",
    "y_real = df['label'].tolist()\n",
    "print(f\"accuracy_score: {accuracy_score(y_test, y_real)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/10:\t1.0\n",
      " 2/10:\t1.0\n",
      " 3/10:\t0.8095238095238095\n",
      " 4/10:\t0.8095238095238095\n",
      " 5/10:\t0.7619047619047619\n",
      " 6/10:\t0.8095238095238095\n",
      " 7/10:\t0.9047619047619048\n",
      " 8/10:\t0.9047619047619048\n",
      " 9/10:\t1.0\n",
      "10/10:\t1.0\n",
      "mean:\t0.9\n"
     ]
    }
   ],
   "source": [
    "# [evalA]: preprocessed, omitting stop words\n",
    "# evaluation using k-fold with k=10\n",
    "X = df['preprocessed_data']\n",
    "y = df['label']\n",
    "kf = KFold(n_splits=10, shuffle=False)\n",
    "\n",
    "# 16 stop words to omit\n",
    "stop_words = ['aeu', 'all', 'bylademsbt', 'blz', 'bp', 'ccbadexxx', 'ga', 'gaa', \\\n",
    "              'geb', 'rfalld', 'spdudexxx', 'sskndexxx', 'to', 'ts', 'ue', 'vag']\n",
    "\n",
    "accuracy_scores = []\n",
    "\n",
    "for ind, data in enumerate(kf.split(X)):\n",
    "    train_index, test_index = data \n",
    "    print(f\"{str(ind + 1).rjust(2)}/{kf.n_splits}:\\t\", end='')\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    count_vectorizer = CountVectorizer(lowercase=True, stop_words=stop_words)\n",
    "    count_vectorizer.fit(X_train)\n",
    "    vectorized_messages = count_vectorizer.transform(X_train).toarray()\n",
    "    gnb = GaussianNB()\n",
    "    gnb = gnb.fit(vectorized_messages, y_train)\n",
    "    result = gnb.predict(count_vectorizer.transform(X_test).toarray())\n",
    "    acc_score = accuracy_score(result, y_test.tolist())\n",
    "    print(acc_score)\n",
    "    accuracy_scores.append(acc_score)\n",
    "\n",
    "print(f\"mean:\\t{mean(accuracy_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/10:\t1.0\n",
      " 2/10:\t1.0\n",
      " 3/10:\t0.8095238095238095\n",
      " 4/10:\t0.8571428571428571\n",
      " 5/10:\t0.8095238095238095\n",
      " 6/10:\t0.9047619047619048\n",
      " 7/10:\t0.8571428571428571\n",
      " 8/10:\t1.0\n",
      " 9/10:\t1.0\n",
      "10/10:\t1.0\n",
      "mean:\t0.9238095238095239\n"
     ]
    }
   ],
   "source": [
    "# [evalB]: no-preprocessing, no stop words\n",
    "# evaluation using k-fold with k=10\n",
    "X = df['accumulated_data']\n",
    "y = df['label']\n",
    "kf = KFold(n_splits=10, shuffle=False)\n",
    "\n",
    "accuracy_scores = []\n",
    "\n",
    "for ind, data in enumerate(kf.split(X)):\n",
    "    train_index, test_index = data \n",
    "    print(f\"{str(ind + 1).rjust(2)}/{kf.n_splits}:\\t\", end='')\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    count_vectorizer = CountVectorizer(lowercase=True)\n",
    "    count_vectorizer.fit(X_train)\n",
    "    vectorized_messages = count_vectorizer.transform(X_train).toarray()\n",
    "    gnb = GaussianNB()\n",
    "    gnb = gnb.fit(vectorized_messages, y_train)\n",
    "    result = gnb.predict(count_vectorizer.transform(X_test).toarray())\n",
    "    acc_score = accuracy_score(result, y_test.tolist())\n",
    "    print(acc_score)\n",
    "    accuracy_scores.append(acc_score)\n",
    "\n",
    "print(f\"mean:\\t{mean(accuracy_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/10:\t1.0\n",
      " 2/10:\t1.0\n",
      " 3/10:\t0.7619047619047619\n",
      " 4/10:\t0.8095238095238095\n",
      " 5/10:\t0.7619047619047619\n",
      " 6/10:\t0.7142857142857143\n",
      " 7/10:\t0.9047619047619048\n",
      " 8/10:\t0.8095238095238095\n",
      " 9/10:\t1.0\n",
      "10/10:\t1.0\n",
      "mean:\t0.8761904761904762\n"
     ]
    }
   ],
   "source": [
    "# [evalC]: preprocessing, omitting stop words (including 'eu', 'eur' and 'euro')\n",
    "# evaluation using k-fold with k=10\n",
    "X = df['preprocessed_data']\n",
    "y = df['label']\n",
    "kf = KFold(n_splits=10, shuffle=False)\n",
    "\n",
    "# 19 stop words to omit\n",
    "stop_words = ['aeu', 'all', 'bylademsbt', 'blz', 'bp', 'ccbadexxx', 'eu', 'eur', 'euro', \\\n",
    "              'ga', 'gaa', 'geb', 'rfalld', 'spdudexxx', 'sskndexxx', 'to', 'ts', 'ue', 'vag']\n",
    "\n",
    "accuracy_scores = []\n",
    "\n",
    "for ind, data in enumerate(kf.split(X)):\n",
    "    train_index, test_index = data \n",
    "    print(f\"{str(ind + 1).rjust(2)}/{kf.n_splits}:\\t\", end='')\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    count_vectorizer = CountVectorizer(lowercase=True, stop_words=stop_words)\n",
    "    count_vectorizer.fit(X_train)\n",
    "    vectorized_messages = count_vectorizer.transform(X_train).toarray()\n",
    "    gnb = GaussianNB()\n",
    "    gnb = gnb.fit(vectorized_messages, y_train)\n",
    "    result = gnb.predict(count_vectorizer.transform(X_test).toarray())\n",
    "    acc_score = accuracy_score(result, y_test.tolist())\n",
    "    print(acc_score)\n",
    "    accuracy_scores.append(acc_score)\n",
    "\n",
    "print(f\"mean:\\t{mean(accuracy_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7142857142857143\n",
      "['income' 'living' 'private' 'standardOfLiving' 'leisure' 'finance']\n",
      "[[1 0 0 0 0 0]\n",
      " [0 3 0 1 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 3 0 0]\n",
      " [0 1 0 4 7 0]\n",
      " [0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# investigating the worst split of [evalC] \n",
    "train_index, test_index = list(kf.split(X))[5]\n",
    "X_train, X_test = X[train_index], X[test_index]\n",
    "y_train, y_test = y[train_index], y[test_index]\n",
    "count_vectorizer = CountVectorizer(lowercase=True, stop_words=stop_words)\n",
    "count_vectorizer.fit(X_train)\n",
    "vectorized_messages = count_vectorizer.transform(X_train).toarray()\n",
    "gnb = GaussianNB()\n",
    "gnb = gnb.fit(vectorized_messages, y_train)\n",
    "result = gnb.predict(count_vectorizer.transform(X_test).toarray())\n",
    "acc_score = accuracy_score(result, y_test.tolist())\n",
    "print(acc_score)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(df.label.unique())\n",
    "print(confusion_matrix(y_test, result, labels=df.label.unique()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
